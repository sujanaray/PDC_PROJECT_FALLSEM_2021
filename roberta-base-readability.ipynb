{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T19:04:02.426362Z","iopub.execute_input":"2021-12-01T19:04:02.426572Z","iopub.status.idle":"2021-12-01T19:04:02.447153Z","shell.execute_reply.started":"2021-12-01T19:04:02.426548Z","shell.execute_reply":"2021-12-01T19:04:02.446413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()\n\nimport torch\nimport torch.nn as nn #nn class to make code more concise\nimport torch.nn.functional as F #contains all functions of nn class as well as a wide range of activation and loss functions\nimport torch.optim as optim #optimization algorithms\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig #For RoBERTa configuration. It is used to instantiate a RoBERTa model according to the specified arguments, defining the model architecture.\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer #byte-level BPE (Byte Pair Encoding) for tokenizer\nfrom transformers import RobertaModel #Initializing the model\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:03:56.419513Z","iopub.execute_input":"2021-12-01T19:03:56.419936Z","iopub.status.idle":"2021-12-01T19:04:02.33013Z","shell.execute_reply.started":"2021-12-01T19:03:56.419807Z","shell.execute_reply":"2021-12-01T19:04:02.329385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv') #Training data set\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv') # Test set","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:02.331643Z","iopub.execute_input":"2021-12-01T19:04:02.331897Z","iopub.status.idle":"2021-12-01T19:04:02.423031Z","shell.execute_reply.started":"2021-12-01T19:04:02.331862Z","shell.execute_reply":"2021-12-01T19:04:02.422275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic data exploration","metadata":{}},{"cell_type":"markdown","source":"#### Trainset","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:07.965224Z","iopub.execute_input":"2021-12-01T19:04:07.965478Z","iopub.status.idle":"2021-12-01T19:04:07.988469Z","shell.execute_reply.started":"2021-12-01T19:04:07.965447Z","shell.execute_reply":"2021-12-01T19:04:07.987754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:13.786643Z","iopub.execute_input":"2021-12-01T19:04:13.786926Z","iopub.status.idle":"2021-12-01T19:04:13.81021Z","shell.execute_reply.started":"2021-12-01T19:04:13.786894Z","shell.execute_reply":"2021-12-01T19:04:13.809438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:15.903698Z","iopub.execute_input":"2021-12-01T19:04:15.903986Z","iopub.status.idle":"2021-12-01T19:04:15.925492Z","shell.execute_reply.started":"2021-12-01T19:04:15.903954Z","shell.execute_reply":"2021-12-01T19:04:15.924747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testset","metadata":{}},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:18.586205Z","iopub.execute_input":"2021-12-01T19:04:18.586654Z","iopub.status.idle":"2021-12-01T19:04:18.59779Z","shell.execute_reply.started":"2021-12-01T19:04:18.586616Z","shell.execute_reply":"2021-12-01T19:04:18.597045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:26.273418Z","iopub.execute_input":"2021-12-01T19:04:26.273676Z","iopub.status.idle":"2021-12-01T19:04:26.284693Z","shell.execute_reply.started":"2021-12-01T19:04:26.273645Z","shell.execute_reply":"2021-12-01T19:04:26.283882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Excerpts (passages) to features\ndef excerpt_to_feature(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '') #Removing \\n\n    \n    #Tokenize and prepare a sequence (of tokens) for the model\n    tok = tokenizer.encode_plus( \n        data, \n        max_length=max_len, \n        truncation=True, #truncate token by token, to a maximum length specified with the argument max_length\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids']) #eg: tok['input_ids'] -> [12, 11, 12038]\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length) #Pad each sentence to the maxi length there is in your batch.\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length) #indicate to the model which part of the inputs correspond to the first sentence (value is 1) and which part corresponds to second sentence/segment (0).\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length) # attention_mask points out which tokens the model should pay attention to (ignore padding tokens). 1 for tokens which are not masked else 0.\n    return curr_sent","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:04:38.012207Z","iopub.execute_input":"2021-12-01T19:04:38.012629Z","iopub.status.idle":"2021-12-01T19:04:38.030629Z","shell.execute_reply.started":"2021-12-01T19:04:38.012589Z","shell.execute_reply":"2021-12-01T19:04:38.029867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retrieve the dataset","metadata":{}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist() #list of excerpt values\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = excerpt_to_feature (\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = excerpt_to_feature (\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:11:34.800655Z","iopub.execute_input":"2021-12-01T19:11:34.800927Z","iopub.status.idle":"2021-12-01T19:11:34.810748Z","shell.execute_reply.started":"2021-12-01T19:11:34.800897Z","shell.execute_reply":"2021-12-01T19:11:34.810098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base RoBERTa Model","metadata":{}},{"cell_type":"code","source":"class ReadabilityModel(nn.Module):\n    def __init__(\n        self, \n        model_name, #roberta-base\n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(ReadabilityModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained( #instantiate the model\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)]) #Reguralization/Scaling: During training, randomly zeroes some of the elements of the input tensor with probability p (0.3)\n        self.regressor = nn.Linear(config.hidden_size, 1) #Applies a linear transformation to the incoming data\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n        \n    #To initialize the layers of the model \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits /= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:18:24.662905Z","iopub.execute_input":"2021-12-01T19:18:24.663651Z","iopub.status.idle":"2021-12-01T19:18:24.680594Z","shell.execute_reply.started":"2021-12-01T19:18:24.663604Z","shell.execute_reply":"2021-12-01T19:18:24.679881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### MakeModel and MakeLoader","metadata":{}},{"cell_type":"code","source":"def make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name) #construct RoBERTa tokenizer\n    config = RobertaConfig.from_pretrained(model_name) #instantiate a RoBERTa model according to the specified arguments\n    config.update({'num_labels':num_labels})\n    model = ReadabilityModel(model_name, config=config) #Initializing the model from the configuration\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset) #Samples elements sequentially, always in the same order.\n    test_loader = DataLoader( #Loads data from the test_dataset\n        test_dataset, #map-style dataset (implements __getitem__())\n        batch_size=batch_size // 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:44:43.070895Z","iopub.execute_input":"2021-12-01T19:44:43.071163Z","iopub.status.idle":"2021-12-01T19:44:43.077703Z","shell.execute_reply.started":"2021-12-01T19:44:43.071133Z","shell.execute_reply":"2021-12-01T19:44:43.077029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"class Evaluation:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast(): #Instances of autocast serve as context managers that allow regions of script to run in mixed precision.\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist() #input ids as list\n                preds += logits #adding list of input ids; list of lists\n        return preds","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:44:47.643452Z","iopub.execute_input":"2021-12-01T19:44:47.643708Z","iopub.status.idle":"2021-12-01T19:44:47.654213Z","shell.execute_reply.started":"2021-12-01T19:44:47.643678Z","shell.execute_reply":"2021-12-01T19:44:47.652329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"def config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test_df, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:49:10.492324Z","iopub.execute_input":"2021-12-01T19:49:10.492582Z","iopub.status.idle":"2021-12-01T19:49:10.500495Z","shell.execute_reply.started":"2021-12-01T19:49:10.492552Z","shell.execute_reply":"2021-12-01T19:49:10.499777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running the model for predictions","metadata":{}},{"cell_type":"code","source":"def run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluation(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:50:32.891322Z","iopub.execute_input":"2021-12-01T19:50:32.891583Z","iopub.status.idle":"2021-12-01T19:50:32.898212Z","shell.execute_reply.started":"2021-12-01T19:50:32.891553Z","shell.execute_reply":"2021-12-01T19:50:32.897495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_base = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_base[f'fold{fold}'] = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:51:09.682476Z","iopub.execute_input":"2021-12-01T19:51:09.682994Z","iopub.status.idle":"2021-12-01T19:51:50.668164Z","shell.execute_reply.started":"2021-12-01T19:51:09.682956Z","shell.execute_reply":"2021-12-01T19:51:50.66746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the predictions to submission file","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsub['target'] = pred_base.mean(axis=1).values.tolist()\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:52:02.12865Z","iopub.execute_input":"2021-12-01T19:52:02.128927Z","iopub.status.idle":"2021-12-01T19:52:02.161378Z","shell.execute_reply.started":"2021-12-01T19:52:02.128887Z","shell.execute_reply":"2021-12-01T19:52:02.160729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Output\nsub","metadata":{"execution":{"iopub.status.busy":"2021-12-01T19:52:04.936234Z","iopub.execute_input":"2021-12-01T19:52:04.936503Z","iopub.status.idle":"2021-12-01T19:52:04.946611Z","shell.execute_reply.started":"2021-12-01T19:52:04.936471Z","shell.execute_reply":"2021-12-01T19:52:04.945725Z"},"trusted":true},"execution_count":null,"outputs":[]}]}